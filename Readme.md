For PPO implementation, we use existing code from MAPPO.
We build the Dual Graph Neural Network & Bi-direction Co-Play training method atop that.